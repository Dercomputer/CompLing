Введение

Салют, Хабр! На связи Игорь Пасечник — технический лид направления XR RnD SberDevices. Сегодня я хочу рассказать про одно из наших направлений исследований — разработку генеративных моделей для 3D-контента.

Современные методы генерации 2D-контента, такие, как 2D-диффузионные модели (Kandinsky 3.0, SDXL), уже достигли впечатляющих результатов и несколько лет являются неотъемлемой частью современности, генеративные видео модели также активно развиваются. Кульминацией развития таких подходов, вероятно, станет представленная не так давно модель Sora. Тем не менее большинство из этих моделей до сих пор испытывают проблемы при генерации консистентных 3D-сцен и объектов.

С другой стороны, существует конвенциональная 3D-графика, а также огромная индустрия и множество прикладных областей, включая игры, XR, дизайн, архитектуру, маркетинг, 3D-проектирование, где используются пайплайны на основе 3D-графики и производится контент на их основе. Методы создания 3D-моделей, такие, как ручное моделирование, 3D-сканирование и фотограмметрия, могут быть трудоёмкими, дорогостоящими и требующими специальных навыков. 3D-продакшн в общем виде использует множество инструментов для создания и рендеринга тяжелой фотореалистичной графики, адаптация генеративных 3D-пайплайнов под такие подходы достаточно тяжела из-за множества инструментов, которые такие пайплайны должны поддерживать. Также адаптация больших латентных генеративных 2D-моделей вроде SORA для прикладных задач фотореалистичной графики может стать альтернативой классическими пайплайнам на основе физического моделирования. Тем не менее, на текущий момент пайплайны работы с графикой, использующие базовый набор примитивов, включая меши, PBR-текстуры, простые модели освещения, закрывают множество прикладных задач и также могут быть востребованы у массового пользователя в случае их демократизации.

Учитывая это, мы решили свести свои исследования в области генеративного 3D к решению нескольких задач:

    Научиться генерировать консистентные multi-view изображения объектов по тексту или картинкам, используя 2D-генеративные модели.

    Научиться генерировать 3D-ассеты для практического использования по тексту или картинкам.

    Научиться делать быструю и достаточно качественную 3D-реконструкцию.

    Научиться создавать консистентные визуализации объектов (3D-гифки).

Мы разработали семейство генеративных 3D-моделей на основе 2D-моделей Сбера, проведя работу с данными, выбрав лучшие компоненты из доступных на текущий момент архитектур, а также внеся дополнительные новшества, достигнув SOTA-результатов относительно open-source и проприетарных моделей.
Общая архитектура пайплайна (Text-to-3D)

На текущий момент существует несколько методов быстрой генерации 3D-контента по тексту:

    С помощью обучения single-view реконструктора можно получать 3D-объект из любой модели генерации картинок (Kandinsky, Stable Diffusion и прочие). Данный реконструктор может быть feed-forward-моделью как в LRM без вариативности в генерации новых ракурсов, либо через диффузию, как DMV3D с возможностью обусловливания на текст напрямую, либо с обусловливанием на изображение.

    По аналогии можно обучить реконструктор sparse-view и использовать диффузионную модель генерации картинок, дообученную на генерацию sparse-view, для получения 3D-объектов. Свой sparse-view генератор изображений представили в ImageDream, MVDream, Instant3D, SPAD, CRM, Wonder3D. В реконструкторе sparse-view модели были обучены с разными рендерингами: volumetric rendering используется в трансформерной модели Instant3D, PF-LRM, гауссовский рендеринг используется в GRM, LGM, mesh-based рендеринг flexicubes используется в CRM.

    В качестве генераторов dense-view используют диффузионные модели для видео, обученные генерировать облеты объектов в работах V3D, VFusion3D, SV3D. Для данных dense-view генераций уже необязательно использовать какие-то дополнительные реконструкционные модели.

    Также долгое время в качестве текст-3D генераторов использовались модели на основе дистилляции текстового промпта через SDS loss. Данные методы работали по-объектно и занимали большое время на генерацию. Недавно было представлено несколько моделей на основе дистилляции ATT3D, AToM и LATTE3D, обученные на множестве промптов одновременно без использования 3D-данных в качестве референса.

    Также есть подходы, которые учат текст-3D модели напрямую, без использования картиночных или видео генераторов. Один из таких подходов GVGEN учится на подсете из 36 тыс. объектов обжаверса, использует гауссовый рендеринг, для которого как раз и нужно иметь 3D-объект, а не его рендеринг.

Изучив все возможные варианты, мы пришли к выводу, что для нашей задачи оптимальным является использование двухстадийного подхода:

    Модель первого этапа: дообученная 2D-диффузионная модель (SDXL/Kandinsky 3.0) для генерации консистентных multi-view картинок. Это позволяет сохранить широкий спектр генерируемых образов и визуальное качество изображений, унаследованных от базовой модели.

    Модель второго этапа: обучение универсальных моделей реконструкторов для генерации конечного 3D-представления, которые могут работать как на сгенерированных картинках из первого этапа, так и для реконструкции объектов по реальным фотографиям.

В рамках такого подхода можно выделить четыре относительно независимых компонента:

    модель первого этапа;

    реконструирующий генератор;

    латентное 3D-представление;

    алгоритм дифференцируемого рендерера.

При создании 3D-объектов мы хотели дать модели всю необходимую информацию о сцене, исключив неопределенности в аспекте геометрии и текстуры. Для получения достаточной информации о генерируемом объекте требуется иметь несколько фотографий сцены с нескольких точек обзора с оптимальными параметрами расположения камер для перекрытия областей обзора. Мы пришли к выводу, что в контексте наших задач не имеет смысла использовать более четырёх ортогональных точек зрения на объект, поэтому мы дообучаем 2D-диффузионную модель для генерации четырех ортогональных ракурсов объекта. Это количество точек обзора является достаточным для реконструкции сцены и сохранения детализации текстур, текущее разрешение синтеза лучших моделей (1024 × 1024) позволяет разделить его на сетку 2 × 2, обеспечивая всё ещё приемлемое качество каждого отдельного вида объекта (512 × 512). Вторым этапом модели является реконструктор, который, используя четыре поступивших на вход изображения, создаёт промежуточное представление сцены в виде триплейнов — ортогональных латентных плоскостей. Наиболее ресурсоёмкой операцией при таком подходе является диффузионная модель. Применив дистилляцию к зафайнтюненой модели первого этапа, мы можем сократить общее время созданий 3D-ассета до 4 секунд.

Рассматривая данный подход, финальное качество работы модели будет определяться следующими факторам:

    Качеством модели первого этапа, уровнем её детализации;

    Качеством работы реконструктора;

    Разрешающей способностью 3D-представления;

    Разрешающей способностью и экспрессивностью рендерера.

Для скалирования подобных моделей необходимо пропорционально улучшать все 4 фактора.
Рассмотрим поподробнее все этапы обучения, а также то, что нам показалось важным.
Первый этап. Text to consistent multi-view
Данные

Так как размеры 3D-датасетов (GSO, ABO, Shapenet, Objaverse, OmniObject3D) не так велики, как используемые для обучения 2D-диффузионных моделей (LAION), то надо попытаться выжать из них всё. В первую очередь для подготовки данных к этому этапу обучения необходимо обработать с помощью пайплайна рендеринга используемые объекты с четырёх ортогональных направлений. Далее для каждой получившейся картинки получить текстовое описание через модель-разметчик. Оптимальным вариантом нам показалось использовать LLaVA-1.5 и синтезировать информацию из нескольких описаний в единое лаконичное описание сцены. Для более удобного отбора данных мы рассчитали значения CLIP и оценки эстетичности от предсказателя эстетики (обученного на классификаторе LAION-aesthetics). Также мы составили небольшую выборку 3D-моделей, разделили их на две категории «красивые» и «некрасивые», объединили свойства CLIP и обучили небольшой SVM-классификатор. Учитывая, что мы используем на данном этапе синтетические данные, ключевой особенностью, которая редко рассматривается в академической литературе, являются параметры рендеринга освещения материалов. Регулируя эти параметры, мы можем влиять на качество и особенности финальной модели, а также эстетического восприятия пользователем генерируемых объектов.
Обучение

Процесс дообучения модели аналогичен обучению основной модели: на вход модели подаются пары «текст-изображение», представленные в виде сетки из четырёх рендеров объекта в определённом порядке. Эти данные проходят через текстовый и визуальный энкодеры, результатом чего становятся текстовый эмбеддинг и эмбеддинг изображения. Затем представление изображения зашумляется в соответствии с конфигурацией расписания диффузии и передаётся на вход стандартной модели типа U-Net с условным входом в виде текстового эмбеддинга. На выходе получается прогноз модели об уровне зашумления латентного представления.

После завершения этапа обучения в течение 5 часов на 16 GPU мы получаем модель, которая способна на основе текстового описания синтезировать один и тот же объект или сцену с нескольких ракурсов, предоставляя полную информацию о пространственном представлении будущей 3D-модели.
Благодаря тому, что мы переиспользовали диффузионную модель и дообучали её на небольшом датасете, она не растеряла способности генерировать объекты не из обучающей выборки, однако выучила новую способность — теперь она умеет видеть их с новой стороны. В процессе обучения происходит несколько процессов — смещение модели в сторону стиля, заданного синтетическим данными, деградация исходного разнообразия модели, снижение степени соответствия генерируемых изображений промпту, а также увеличение степени консистентности четырех проекций объектов между собой. При длительной оптимизации  модель сходится к максимальным 3D консистеным объектам, но при этом падает вариативность геометрии и текстур, поэтому локальный минимум с наилучшим балансом всех факторов достаточно узок и сильно зависит от гиперпараметров, а также оптимального набора данных. Для валидации и выбора оптимальной итерации можно использовать ошибку реконструкции модели второго этапа а также CLIP score.
Механизмы контроля генерации

Несмотря на успешность дообучения модели, часто требуется дополнительное управление моделью, чтобы гарантировать, что на итоговом изображении четыре синтезированных рендера будут разделены и каждый будет соответствовать проекции обособленного объекта, а не сцены с явно выраженным диффузионным фоном. Для этого приходится прибегать к различным хитростям. Например, можно использовать изображение, состоящее из 4 равномерно распределенных 2D-гауссиан для формирования стартовой точки при генерации итоговых изображений. После создания такого исходного изображения необходимо получить его латентное представление и добавить шум в соответствии с расписанием диффузионного процесса. Этим же методом можно контролировать размер объекта.
Второй этап. Реконструтор
Общий процесс обучения


В реконструируемом этапе общая идея модели, которую мы используем, выглядит следующим образом:

    На вход мы принимаем 4 картинки объекта с разных ракурсов и с помощью реконструируемого генератора получаем из этих картинок triplane-представление, 3 feature-плоскости, на которые проецируется объект.

    Затем мы семплируем точки в 3D-пространстве в зависимости от используемой модели рендеринга, получаем соответствующие этим точкам features из триплейнов и декодируем из них с помощью MLP в sdf/opacity/tex_features, которые потом будут использоваться в рендеринге.

    Рендерим объект из полученного триплейна с четырёх случайных ракурсов, взятых из датасета, оптимизируем с лоссом LPIPS + mse между отрендеренными изображениями и изображениями из датасета, соответствующим этим ракурсам.

В рамках такой схемы мы рассмотрели два вида архитектур генераторов и несколько архитектур рендерера.
Генератор триплейна на основе трансформера (instant3d)

Одной из первых работ, использующих такой подход к реконструкции, является Instant3d, она же на текущий момент демонстрирует одни из лучших результатов по синтетическим метрикам.
Однако базовая архитектура имеет ряд недостатков, не указанных в статье, а именно — ограниченную детализацию текстуры, блочные артефакты для высокочастотных регионов, низкое разрешение выходного триплейна. Модифицировав исходную архитектуру, включая расширение внутреннего представления генератора до 64 × 64, использовав DINOv2 энкодеры с регистрами, а также более сложный upsampler, мы смогли получить лучшие результаты и избавиться от артефактов, однако общая архитектура модели вносит ограничения в масштабируемость такого подхода, потому что увеличение количества параметров генератора не влияет в явном виде на детализацию текстур на объекте, а лишь на способность генератора фьюзить входные данные в целевое представление объекта.